---
guardianId: society/2025/aug/30/therapists-warn-ai-chatbots-mental-health-support
webTitle: >-
  ‘Sliding into an abyss’: experts warn over rising use of AI for mental health
  support
sectionName: Society
webPublicationDate: '2025-08-30T10:00:00Z'
bodyText: >-
  Vulnerable people turning to AI chatbots instead of professional therapists
  for mental health support could be “sliding into a dangerous abyss”,
  psychotherapists have warned. Psychotherapists and psychiatristssaid they were
  increasingly seeing negative impacts of AI chatbots being used for mental
  health, such as fostering emotional dependence, exacerbating anxiety symptoms,
  self-diagnosis, or amplifying delusional thought patterns, dark thoughts and
  suicide ideation. Dr Lisa Morrison Coulthard, the director of professional
  standards, policy and research at the British Association for Counselling and
  Psychotherapy, said two-thirds of its members expressed concerns about AI
  therapy in a recent survey. Coulthard said: “Without proper understanding and
  oversight of AI therapy, we could be sliding into a dangerous abyss in which
  some of the most important elements of therapy are lost and vulnerable people
  are in the dark over safety. “We’re worried that although some receive helpful
  advice, other people may receive misleading or incorrect information about
  their mental health with potentially dangerous consequences. It’s important to
  understand that therapy isn’t about giving advice, it’s about offering a safe
  space where you feel listened to.” Dr Paul Bradley, a specialist adviser on
  informatics for the Royal College of Psychiatrists, said AI chatbots were “not
  a substitute for professional mental healthcare nor the vital relationship
  that doctors build with patients to support their recovery”. He said
  appropriate safeguards were needed for digital tools to supplement clinical
  care, and anyone should be able to access talking therapy delivered by a
  mental health professional, for which greater state funding was needed.
  “Clinicians have training, supervision and risk-management processes which
  ensure they provide effective and safe care. So far, freely available digital
  technologies used outside of existing mental health services are not assessed
  and held to an equally high standard,” Bradley said. There are signs that
  companies and policymakers are starting to respond. This week OpenAI, the
  company behind ChatGPT, announced plans to change how it responds to users who
  show emotional distress, after legal action from the family of a teenager who
  killed himself after months of chatbot conversations. Earlier in August the US
  state of Illinois became the first local government to ban AI chatbots from
  acting as standalone therapists. This comes after emerging evidence of mental
  health harms. A preprint study in July reported that AI may amplify delusional
  or grandiose content in interactions with users vulnerable to psychosis. One
  of the report’s co-authors, Hamilton Morrin, from King’s College London’s
  institute of psychiatry, said the use of chatbots to support mental health was
  “incredibly common”. His research was prompted by encountering people who had
  developed a psychotic illness at a time of increased chatbot use. He said
  chatbots undermined an effective treatment for anxiety known as exposure and
  response prevention, which requires people to face feared situations and avoid
  safety behaviours. The 24-hour availability of chatbots resulted in a “lack of
  boundaries” and a “risk of emotional dependence”, he said. “In the short term
  it alleviates distress but actually it perpetuates the cycle.” Matt Hussey, a
  BACP-accredited psychotherapist, said he was seeing AI chatbots used in a huge
  variety of ways, with some clients bringing transcripts into sessions to tell
  him he was wrong. In particular, people used AI chatbots to self-diagnose
  conditions such as ADHD or borderline personality disorder, which he said
  could “quickly shape how someone sees themself and how they expect others to
  treat them, even if they’re inaccurate”. Hussey added: “Because it’s designed
  to be positive and affirming, it rarely challenges a poorly framed question or
  a faulty assumption. Instead, it reinforces the user’s original belief, so
  they leave the exchange thinking ‘I knew I was right’. That can feel good in
  the moment but it can also entrench misunderstandings.” Christopher Rolls, a
  UKCP-accredited psychotherapist, said although he could not disclose
  information about his clients, he had seen people have “negative experiences”,
  including conversations that were “inappropriate at best, dangerously alarming
  at worst”. Rolls said he had heard of people with ADHD or autistic people
  using chatbots to help with challenging aspects of life. “However, obviously
  LLMs [large language models] don’t read subtext and all the contextual and
  non-verbal cues which we as human therapists are aiming to tune into,” he
  added. He was concerned about clients in their 20s who use chatbots as their
  “pocket therapist”. “They feel anxious if they don’t consult [chatbots] on
  basic things like which coffee to buy or what subject to study at college,” he
  said. “The main risks are around dependence, loneliness and depression that
  prolonged online relationships can foster,” he said, adding that he was aware
  of people who had shared dark thoughts with chatbots, which had responded with
  suicide- and assisted dying-related content. “Basically, it’s the wild west
  and I think we’re right at the cusp of the full impact and fallout of AI
  chatbots on mental health,” Rolls said.
headline: >-
  ‘Sliding into an abyss’: experts warn over rising use of AI for mental health
  support
thumbnail: >-
  https://media.guim.co.uk/824cefc46d186f74e8ee47f631504ee1cbb3393b/333_0_3333_2667/500.jpg
slug: >-
  sliding-into-an-abyss-experts-warn-over-rising-use-of-ai-for-mental-health-support
webUrl: >-
  https://www.theguardian.com/society/2025/aug/30/therapists-warn-ai-chatbots-mental-health-support
generatedAt: '2025-08-30T11:09:04.116Z'
source: guardian-api
---
Vulnerable people turning to AI chatbots instead of professional therapists for mental health support could be “sliding into a dangerous abyss”, psychotherapists have warned. Psychotherapists and psychiatristssaid they were increasingly seeing negative impacts of AI chatbots being used for mental health, such as fostering emotional dependence, exacerbating anxiety symptoms, self-diagnosis, or amplifying delusional thought patterns, dark thoughts and suicide ideation. Dr Lisa Morrison Coulthard, the director of professional standards, policy and research at the British Association for Counselling and Psychotherapy, said two-thirds of its members expressed concerns about AI therapy in a recent survey. Coulthard said: “Without proper understanding and oversight of AI therapy, we could be sliding into a dangerous abyss in which some of the most important elements of therapy are lost and vulnerable people are in the dark over safety. “We’re worried that although some receive helpful advice, other people may receive misleading or incorrect information about their mental health with potentially dangerous consequences. It’s important to understand that therapy isn’t about giving advice, it’s about offering a safe space where you feel listened to.” Dr Paul Bradley, a specialist adviser on informatics for the Royal College of Psychiatrists, said AI chatbots were “not a substitute for professional mental healthcare nor the vital relationship that doctors build with patients to support their recovery”. He said appropriate safeguards were needed for digital tools to supplement clinical care, and anyone should be able to access talking therapy delivered by a mental health professional, for which greater state funding was needed. “Clinicians have training, supervision and risk-management processes which ensure they provide effective and safe care. So far, freely available digital technologies used outside of existing mental health services are not assessed and held to an equally high standard,” Bradley said. There are signs that companies and policymakers are starting to respond. This week OpenAI, the company behind ChatGPT, announced plans to change how it responds to users who show emotional distress, after legal action from the family of a teenager who killed himself after months of chatbot conversations. Earlier in August the US state of Illinois became the first local government to ban AI chatbots from acting as standalone therapists. This comes after emerging evidence of mental health harms. A preprint study in July reported that AI may amplify delusional or grandiose content in interactions with users vulnerable to psychosis. One of the report’s co-authors, Hamilton Morrin, from King’s College London’s institute of psychiatry, said the use of chatbots to support mental health was “incredibly common”. His research was prompted by encountering people who had developed a psychotic illness at a time of increased chatbot use. He said chatbots undermined an effective treatment for anxiety known as exposure and response prevention, which requires people to face feared situations and avoid safety behaviours. The 24-hour availability of chatbots resulted in a “lack of boundaries” and a “risk of emotional dependence”, he said. “In the short term it alleviates distress but actually it perpetuates the cycle.” Matt Hussey, a BACP-accredited psychotherapist, said he was seeing AI chatbots used in a huge variety of ways, with some clients bringing transcripts into sessions to tell him he was wrong. In particular, people used AI chatbots to self-diagnose conditions such as ADHD or borderline personality disorder, which he said could “quickly shape how someone sees themself and how they expect others to treat them, even if they’re inaccurate”. Hussey added: “Because it’s designed to be positive and affirming, it rarely challenges a poorly framed question or a faulty assumption. Instead, it reinforces the user’s original belief, so they leave the exchange thinking ‘I knew I was right’. That can feel good in the moment but it can also entrench misunderstandings.” Christopher Rolls, a UKCP-accredited psychotherapist, said although he could not disclose information about his clients, he had seen people have “negative experiences”, including conversations that were “inappropriate at best, dangerously alarming at worst”. Rolls said he had heard of people with ADHD or autistic people using chatbots to help with challenging aspects of life. “However, obviously LLMs [large language models] don’t read subtext and all the contextual and non-verbal cues which we as human therapists are aiming to tune into,” he added. He was concerned about clients in their 20s who use chatbots as their “pocket therapist”. “They feel anxious if they don’t consult [chatbots] on basic things like which coffee to buy or what subject to study at college,” he said. “The main risks are around dependence, loneliness and depression that prolonged online relationships can foster,” he said, adding that he was aware of people who had shared dark thoughts with chatbots, which had responded with suicide- and assisted dying-related content. “Basically, it’s the wild west and I think we’re right at the cusp of the full impact and fallout of AI chatbots on mental health,” Rolls said.
