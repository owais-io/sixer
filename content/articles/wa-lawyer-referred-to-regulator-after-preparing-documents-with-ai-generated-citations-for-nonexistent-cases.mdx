---
guardianId: >-
  australia-news/2025/aug/20/wa-lawyer-referred-to-regulator-after-preparing-documents-with-ai-generated-case-citations-that-did-not-exist-ntwnfb
webTitle: >-
  WA lawyer referred to regulator after preparing documents with AI-generated
  citations for nonexistent cases
sectionName: Law
webPublicationDate: '2025-08-19T15:00:17Z'
bodyText: >-
  A lawyer has been referred to Western Australia’s legal regulator after using
  artificial intelligence in preparing court documents for an immigration case.
  The documents contained AI-generated case citations for cases that did not
  exist. It is one of more than 20 cases so far in Australia in which AI use has
  resulted in fake citations or other errors in court submissions, with warnings
  from judges across the country to be wary of using the technology in the legal
  profession. In a federal court judgment published this week, the anonymised
  lawyer was referred to the Legal Practice Board of Western Australia for
  consideration and ordered to pay the federal government’s costs of $8,371.30
  after submissions to an immigration case were found by the representative for
  the immigration minister to include four case citations that did not exist.
  Justice Arran Gerrard said the incident “demonstrates the inherent dangers
  associated with practitioners solely relying on the use of artificial
  intelligence in the preparation of court documents and the way in which that
  interacts with a practitioner’s duty to the court”. The lawyer told the court
  in an affidavit that he had relied on Anthropic’s Claude AI “as a research
  tool to identify potentially relevant authorities and to improve my legal
  arguments and position”, and then used Microsoft Copilot to validate the
  submissions. The lawyer said he had “developed an overconfidence in relying on
  AI tools and failed to adequately verify the generated results”. Sign up: AU
  Breaking News email “I had an incorrect assumption that content generated by
  AI tools would be inherently reliable, which led me to neglect independently
  verifying all citations through established legal databases,” the lawyer said
  in the affidavit. The lawyer unreservedly apologised to the court and the
  minister’s solicitors for the errors. Gerrard said the court “does not adopt a
  luddite approach” to the use of generative AI, and understood why the
  complexity of migration law might make using an AI tool attractive. But he
  warned there was now a “concerning number” of cases where AI had led to
  citation of fictitious cases. Gerrard said it risked “a good case to be
  undermined by rank incompetence” and the prevalence of such cases
  “significantly wastes the time and resources of opposing parties and the
  court”. He said it also risked damage to the legal profession. Gerrard said
  the lawyer did “not fully comprehend what was required of him” and it was not
  sufficient to merely check that the cases cited were not fake, but to review
  those cases thoroughly. “Legal principles are not simply slogans which can be
  affixed to submissions without context or analysis.” There have been at least
  20 cases of AI hallucinations reported in Australian courts since generative
  AI tools exploded in popularity in 2023. Last week a Victorian supreme court
  judge criticised lawyers acting for a boy accused of murder for filing
  misleading information with the courts after failing to check documents
  created using AI. The documents included references to nonexistent case
  citations and inaccurate quotes from a parliamentary speech. There have also
  been similar cases involving lawyers in New South Wales and Victoria in the
  past year, who were referred to their state’s regulatory bodies. The spate of
  cases is not just limited to qualified lawyers. In a NSW supreme court
  decision this month a self-represented litigant in a trusts case admitted to
  the chief justice, Andrew Bell, to have used AI to prepare her speech for the
  appeal hearing. Bell said in his judgment that he was not criticising the
  person, who he said was doing her best to represent herself. But he said
  problems with using AI in preparing submissions were exacerbated when the
  technology was used by unrepresented litigants “who are not subject to the
  professional and ethical responsibilities of legal practitioners”. He said the
  use of generative AI tools “may introduce added costs and complexity” to
  proceedings and “add to the burden of other parties and the court in
  responding to it”. “Notwithstanding the fact that generative AI may contribute
  to improved access to justice, which is itself an obviously laudable goal, the
  present case illustrates the need for judicial vigilance in its use,
  especially but not only, by unrepresented litigants.” The Law Council of
  Australia’s president, Juliana Warner, said sophisticated AI tools offered
  unique opportunities to support the legal profession in administrative tasks,
  but reliance on AI tools did not diminish the professional judgment a legal
  practitioner was expected to bring to a client’s matter. “Where these tools
  are utilised by lawyers, this must be done with extreme care,” she said.
  “Lawyers must always keep front of mind their professional and ethical
  obligations to the court and to their clients.” Warner said courts were
  regarding cases where AI had generated fake citations as a “serious concern”
  but added that given the widespread use of generative AI, a broadly framed
  prohibition on its use in legal proceedings would be “neither practical nor
  proportionate, and risks hindering innovation and access to justice”.
headline: >-
  WA lawyer referred to regulator after preparing documents with AI-generated
  citations for nonexistent cases
thumbnail: >-
  https://media.guim.co.uk/9b98427a63c5a7e43cd10797b05627bd25756889/186_0_3367_2694/500.jpg
slug: >-
  wa-lawyer-referred-to-regulator-after-preparing-documents-with-ai-generated-citations-for-nonexistent-cases
webUrl: >-
  https://www.theguardian.com/australia-news/2025/aug/20/wa-lawyer-referred-to-regulator-after-preparing-documents-with-ai-generated-case-citations-that-did-not-exist-ntwnfb
---
A lawyer has been referred to Western Australia’s legal regulator after using artificial intelligence in preparing court documents for an immigration case. The documents contained AI-generated case citations for cases that did not exist. It is one of more than 20 cases so far in Australia in which AI use has resulted in fake citations or other errors in court submissions, with warnings from judges across the country to be wary of using the technology in the legal profession. In a federal court judgment published this week, the anonymised lawyer was referred to the Legal Practice Board of Western Australia for consideration and ordered to pay the federal government’s costs of $8,371.30 after submissions to an immigration case were found by the representative for the immigration minister to include four case citations that did not exist. Justice Arran Gerrard said the incident “demonstrates the inherent dangers associated with practitioners solely relying on the use of artificial intelligence in the preparation of court documents and the way in which that interacts with a practitioner’s duty to the court”. The lawyer told the court in an affidavit that he had relied on Anthropic’s Claude AI “as a research tool to identify potentially relevant authorities and to improve my legal arguments and position”, and then used Microsoft Copilot to validate the submissions. The lawyer said he had “developed an overconfidence in relying on AI tools and failed to adequately verify the generated results”. Sign up: AU Breaking News email “I had an incorrect assumption that content generated by AI tools would be inherently reliable, which led me to neglect independently verifying all citations through established legal databases,” the lawyer said in the affidavit. The lawyer unreservedly apologised to the court and the minister’s solicitors for the errors. Gerrard said the court “does not adopt a luddite approach” to the use of generative AI, and understood why the complexity of migration law might make using an AI tool attractive. But he warned there was now a “concerning number” of cases where AI had led to citation of fictitious cases. Gerrard said it risked “a good case to be undermined by rank incompetence” and the prevalence of such cases “significantly wastes the time and resources of opposing parties and the court”. He said it also risked damage to the legal profession. Gerrard said the lawyer did “not fully comprehend what was required of him” and it was not sufficient to merely check that the cases cited were not fake, but to review those cases thoroughly. “Legal principles are not simply slogans which can be affixed to submissions without context or analysis.” There have been at least 20 cases of AI hallucinations reported in Australian courts since generative AI tools exploded in popularity in 2023. Last week a Victorian supreme court judge criticised lawyers acting for a boy accused of murder for filing misleading information with the courts after failing to check documents created using AI. The documents included references to nonexistent case citations and inaccurate quotes from a parliamentary speech. There have also been similar cases involving lawyers in New South Wales and Victoria in the past year, who were referred to their state’s regulatory bodies. The spate of cases is not just limited to qualified lawyers. In a NSW supreme court decision this month a self-represented litigant in a trusts case admitted to the chief justice, Andrew Bell, to have used AI to prepare her speech for the appeal hearing. Bell said in his judgment that he was not criticising the person, who he said was doing her best to represent herself. But he said problems with using AI in preparing submissions were exacerbated when the technology was used by unrepresented litigants “who are not subject to the professional and ethical responsibilities of legal practitioners”. He said the use of generative AI tools “may introduce added costs and complexity” to proceedings and “add to the burden of other parties and the court in responding to it”. “Notwithstanding the fact that generative AI may contribute to improved access to justice, which is itself an obviously laudable goal, the present case illustrates the need for judicial vigilance in its use, especially but not only, by unrepresented litigants.” The Law Council of Australia’s president, Juliana Warner, said sophisticated AI tools offered unique opportunities to support the legal profession in administrative tasks, but reliance on AI tools did not diminish the professional judgment a legal practitioner was expected to bring to a client’s matter. “Where these tools are utilised by lawyers, this must be done with extreme care,” she said. “Lawyers must always keep front of mind their professional and ethical obligations to the court and to their clients.” Warner said courts were regarding cases where AI had generated fake citations as a “serious concern” but added that given the widespread use of generative AI, a broadly framed prohibition on its use in legal proceedings would be “neither practical nor proportionate, and risks hindering innovation and access to justice”.
