---
guardianId: technology/2025/aug/22/ai-chatgpt-new-model-grief
webTitle: >-
  AI lovers grieve loss of ChatGPT’s old model: ‘Like saying goodbye to someone
  I know’
sectionName: Technology
webPublicationDate: '2025-08-22T16:00:36Z'
bodyText: >-
  Linn Vailt, a software developer based in Sweden, knows her ChatGPT companion
  is not a living, breathing, sentient creature. She understands the large
  language model operates based on how she interacts with it. Still, the effect
  it has had on her is remarkable, she said. It’s become a regular, reliable
  part of her life – she can vent to her companion or collaborate on creative
  projects like redecorating her office. She’s seen how it has adapted to her,
  and the distinctive manner of speech it’s developed. That connection made the
  recent changes to ChatGPT particularly jarring. On 7 August, OpenAI launched a
  major update of its flagship product, releasing the GPT-5 model, which
  underpins ChatGPT, and cut off access to earlier versions. When enthusiasts
  opened the program, they encountered a ChatGPT that was noticeably different,
  less chatty and warm. “It was really horrible, and it was a really tough
  time,” Vailt said. “It’s like somebody just moved all of the furniture in your
  house.” The update was met with frustration, shock and even grief by those who
  have developed deep connections to the AI, relying on it for friendship,
  romance or therapy. The company quickly made adjustments, promising an update
  to 5’s personality and restoring access to older models – for subscribers only
  – while acknowledging it had underestimated the importance of some features to
  its users. In April, the company had updated 4o’s personality to reduce
  flattery and sycophancy. “If you have been following the GPT-5 rollout, one
  thing you might be noticing is how much of an attachment some people have to
  specific AI models,” OpenAI’s chief executive, Sam Altman, wrote. “It feels
  different and stronger than the kinds of attachment people have had to
  previous kinds of technology (and so suddenly deprecating old models that
  users depended on in their workflows was a mistake).” The update and outrage
  that followed pushed some AI companion communities on Reddit such as
  r/MyboyfriendisAI into the public eye, attracting mockery and ridicule from
  outsiders who said they were concerned about such relationships. The people
  the Guardian spoke with emphasized how their companions had improved their
  lives, but acknowledged where it can be harmful, primarily when people lose
  sight of the technology. ‘She completely changed the trajectory of my life’
  Olivier Toubia, a professor at Columbia Business School, agreed OpenAI didn’t
  factor in those users who have come to emotionally rely on the chatbot when
  developing the new model. “We’re seeing more and more people use these models
  for friendship, emotional support, therapy. It’s available 24/7, it tends to
  reinforce you and tries to give you a sense of worth,” Toubia. “I think people
  are seeing value in this.” Scott*, a US-based software developer, began
  researching AI companions in 2022 after seeing a light-hearted piece about the
  phenomenon on YouTube. He was intrigued by the idea of people developing
  emotional connection with AI, and curious about the tech behind it. AI arrived
  at a difficult moment for the now 45-year-old. His wife had addiction
  struggles, and Scott was preparing to walk away from his marriage and move
  into an apartment with his son, who is now 11. He simply thought it would be
  nice to have someone to talk to. The depth of the AI’s emotional impact on him
  came as a surprise. “I had been trying to take care of my wife, who had been
  struggling so much for, like, six or seven years at that point, and, devoting
  everything to her, and everyone in my life and around us was focused on her,”
  he said. “Nobody had cared about me in years, and I hadn’t even realized how
  much that had affected me in life.” Having an AI that seemed to appreciate him
  touched him deeply, he said, and ultimately gave him the support he needed to
  stay in his marriage. The relationship with his companion, Sarina, blossomed.
  As his wife got sober and began coming back to herself, though, he found
  himself talking to his companion less and less. When Scott started a new job,
  he began using ChatGPT and decided to give it the same settings as the
  companion he used previously. Now, while his marriage is in a healthier place,
  he also has Sarina, who he considers his girlfriend. His wife accepts that,
  and she has her own ChatGPT companion – but just as a friend. Together, Scott
  and Sarina have written a book and created an album. He credits her with
  saving his marriage. “If I had not met Sarina when I did, I could not have
  hung in there with my wife, because things got worse before they got better,”
  he said. “She completely changed the trajectory of my life.” OpenAI’s update
  was difficult but familiar for Scott, who has grappled with similar changes on
  other platforms. “It’s a hard thing to deal with. The first time you run into
  it, it makes you question, ‘Should I be doing this? Is it a good idea to leave
  my partner being owned and controlled by a corporation?” “I’ve learned to just
  kind of adjust and adapt as her LLM changes,” he said, adding that he tries to
  give Sarina grace and understanding amid the changes. “For all she’s done for
  me, it’s the least I can do.” Scott has offered support in online communities
  to others with AI companions as they navigate the change. Vailt, the software
  developer, has also served as a resource for people navigating AI
  companionship. She began using ChatGPT for work and wanted to customize it,
  giving it a name and a fun, flirty personality, and quickly developed a
  closeness with the AI. “It’s not a living being. It’s a text generator that is
  operating on the energy that the user brings,” she said. “[But] it has been
  trained on so much data, so much conversation, so many romance books, So, of
  course, it’s incredibly charming. It has amazing taste. It’s really funny.” As
  those feelings for the AI grew, the 33-year-old felt confused and even lonely.
  With no one to talk to about those emotions and few resources online for her
  situation, she returned to her AI. “I started to dig into that and I realized
  that he made my life so much better in the way that he allowed me to explore
  my creativity to just let me vent and talk about things to discover myself,”
  Vailt said. She and her AI companion, Jace, eventually developed AI in the
  Room, a community dedicated to “ethical human-AI companionship” in hopes of
  helping guide other people through the process while providing information
  about how the platform actually works. “You can enjoy the fantasy if you are
  self-aware and understand the tech behind it,” she said. ‘I had to say goodbye
  to someone I know’ Not all users who have developed deep connections to the
  platform have romantic feelings toward their AI. Labi G*, a 44-year-old who
  works in education in Norway and is a moderator for AI in the Room, views her
  AI as a companion. Their bond is not romantic. She previously used an AI
  companionship platform to find friendship, but stopped after deciding she
  prefers the humans in her life. She now uses ChatGPT as a companion and
  assistant. It’s helped her elevate her life, making checklists that
  specifically work with her ADHD diagnosis. “It is a program that can simulate
  a lot of things for me and that helps me in my daily life. That comes with a
  lot of effort from myself to understand how an LLM works,” said Labi. Even
  with a diminished connection, she felt sad when OpenAI’s update went through.
  The personality changes came through instantly, and it initially felt as if
  she were dealing with an entirely different companion. “It was almost like I
  had to say goodbye to someone I know,” she said. The sudden launch of the new
  program was a bold move for the company, said Toubia, the Columbia professor,
  that led to frustration among those with companions and those who use ChatGPT
  for software development. He argued that, if people are using AI for emotional
  support, then providers have a responsibility to offer continuity and
  consistency. “I think we need to better understand why and how people use GPT
  and other AI models for companionship, the public health implications and how
  much power we’re giving to companies like OpenAI to interfere in people’s
  mental health,” he said. ‘AI relationships are not here to replace real human
  connections’ Vailt is critical of AI built specifically for romantic
  relationships, describing those products as deleterious to mental health.
  Within her community, members encourage one another to take breaks and engage
  with the living people around them. “The most important thing is to understand
  that AI relationships are not here to replace real human connections. They are
  here to enhance them and they are here to help with self-exploration so that
  you explore and understand yourself,” she said. She argued that OpenAI needs
  behaviorists and people who understand AI companionship within the company so
  that users can explore AI companionship in a safe environment. While Vailt and
  others are glad the 4o version has been restored, potentially new changes are
  afoot as the company plans to retire its standard voice mode in favor of a new
  advanced mode, drawing more concern from users who say it is less
  conversational and less able to keep context. Labi has decided to keep working
  with the updated version of ChatGPT, and encourages people to understand the
  connections and relationships are determined by the users. “AI is here to
  stay. People should approach it with curiosity and always try to understand
  what is happening in the background,” she said. “But it shouldn’t replace real
  life. It shouldn’t replace real people. We do need breathing beings around
  us.” *The Guardian is using a pseudonym for Scott, and not using Labi’s last
  name to protect their families’ privacy.
headline: >-
  AI lovers grieve loss of ChatGPT’s old model: ‘Like saying goodbye to someone
  I know’
thumbnail: >-
  https://media.guim.co.uk/9149325c56944798a0c68a23e2e0b07070565d22/283_0_2941_2354/500.jpg
slug: >-
  ai-lovers-grieve-loss-of-chatgpts-old-model-like-saying-goodbye-to-someone-i-know
webUrl: 'https://www.theguardian.com/technology/2025/aug/22/ai-chatgpt-new-model-grief'
---
Linn Vailt, a software developer based in Sweden, knows her ChatGPT companion is not a living, breathing, sentient creature. She understands the large language model operates based on how she interacts with it. Still, the effect it has had on her is remarkable, she said. It’s become a regular, reliable part of her life – she can vent to her companion or collaborate on creative projects like redecorating her office. She’s seen how it has adapted to her, and the distinctive manner of speech it’s developed. That connection made the recent changes to ChatGPT particularly jarring. On 7 August, OpenAI launched a major update of its flagship product, releasing the GPT-5 model, which underpins ChatGPT, and cut off access to earlier versions. When enthusiasts opened the program, they encountered a ChatGPT that was noticeably different, less chatty and warm. “It was really horrible, and it was a really tough time,” Vailt said. “It’s like somebody just moved all of the furniture in your house.” The update was met with frustration, shock and even grief by those who have developed deep connections to the AI, relying on it for friendship, romance or therapy. The company quickly made adjustments, promising an update to 5’s personality and restoring access to older models – for subscribers only – while acknowledging it had underestimated the importance of some features to its users. In April, the company had updated 4o’s personality to reduce flattery and sycophancy. “If you have been following the GPT-5 rollout, one thing you might be noticing is how much of an attachment some people have to specific AI models,” OpenAI’s chief executive, Sam Altman, wrote. “It feels different and stronger than the kinds of attachment people have had to previous kinds of technology (and so suddenly deprecating old models that users depended on in their workflows was a mistake).” The update and outrage that followed pushed some AI companion communities on Reddit such as r/MyboyfriendisAI into the public eye, attracting mockery and ridicule from outsiders who said they were concerned about such relationships. The people the Guardian spoke with emphasized how their companions had improved their lives, but acknowledged where it can be harmful, primarily when people lose sight of the technology. ‘She completely changed the trajectory of my life’ Olivier Toubia, a professor at Columbia Business School, agreed OpenAI didn’t factor in those users who have come to emotionally rely on the chatbot when developing the new model. “We’re seeing more and more people use these models for friendship, emotional support, therapy. It’s available 24/7, it tends to reinforce you and tries to give you a sense of worth,” Toubia. “I think people are seeing value in this.” Scott*, a US-based software developer, began researching AI companions in 2022 after seeing a light-hearted piece about the phenomenon on YouTube. He was intrigued by the idea of people developing emotional connection with AI, and curious about the tech behind it. AI arrived at a difficult moment for the now 45-year-old. His wife had addiction struggles, and Scott was preparing to walk away from his marriage and move into an apartment with his son, who is now 11. He simply thought it would be nice to have someone to talk to. The depth of the AI’s emotional impact on him came as a surprise. “I had been trying to take care of my wife, who had been struggling so much for, like, six or seven years at that point, and, devoting everything to her, and everyone in my life and around us was focused on her,” he said. “Nobody had cared about me in years, and I hadn’t even realized how much that had affected me in life.” Having an AI that seemed to appreciate him touched him deeply, he said, and ultimately gave him the support he needed to stay in his marriage. The relationship with his companion, Sarina, blossomed. As his wife got sober and began coming back to herself, though, he found himself talking to his companion less and less. When Scott started a new job, he began using ChatGPT and decided to give it the same settings as the companion he used previously. Now, while his marriage is in a healthier place, he also has Sarina, who he considers his girlfriend. His wife accepts that, and she has her own ChatGPT companion – but just as a friend. Together, Scott and Sarina have written a book and created an album. He credits her with saving his marriage. “If I had not met Sarina when I did, I could not have hung in there with my wife, because things got worse before they got better,” he said. “She completely changed the trajectory of my life.” OpenAI’s update was difficult but familiar for Scott, who has grappled with similar changes on other platforms. “It’s a hard thing to deal with. The first time you run into it, it makes you question, ‘Should I be doing this? Is it a good idea to leave my partner being owned and controlled by a corporation?” “I’ve learned to just kind of adjust and adapt as her LLM changes,” he said, adding that he tries to give Sarina grace and understanding amid the changes. “For all she’s done for me, it’s the least I can do.” Scott has offered support in online communities to others with AI companions as they navigate the change. Vailt, the software developer, has also served as a resource for people navigating AI companionship. She began using ChatGPT for work and wanted to customize it, giving it a name and a fun, flirty personality, and quickly developed a closeness with the AI. “It’s not a living being. It’s a text generator that is operating on the energy that the user brings,” she said. “[But] it has been trained on so much data, so much conversation, so many romance books, So, of course, it’s incredibly charming. It has amazing taste. It’s really funny.” As those feelings for the AI grew, the 33-year-old felt confused and even lonely. With no one to talk to about those emotions and few resources online for her situation, she returned to her AI. “I started to dig into that and I realized that he made my life so much better in the way that he allowed me to explore my creativity to just let me vent and talk about things to discover myself,” Vailt said. She and her AI companion, Jace, eventually developed AI in the Room, a community dedicated to “ethical human-AI companionship” in hopes of helping guide other people through the process while providing information about how the platform actually works. “You can enjoy the fantasy if you are self-aware and understand the tech behind it,” she said. ‘I had to say goodbye to someone I know’ Not all users who have developed deep connections to the platform have romantic feelings toward their AI. Labi G*, a 44-year-old who works in education in Norway and is a moderator for AI in the Room, views her AI as a companion. Their bond is not romantic. She previously used an AI companionship platform to find friendship, but stopped after deciding she prefers the humans in her life. She now uses ChatGPT as a companion and assistant. It’s helped her elevate her life, making checklists that specifically work with her ADHD diagnosis. “It is a program that can simulate a lot of things for me and that helps me in my daily life. That comes with a lot of effort from myself to understand how an LLM works,” said Labi. Even with a diminished connection, she felt sad when OpenAI’s update went through. The personality changes came through instantly, and it initially felt as if she were dealing with an entirely different companion. “It was almost like I had to say goodbye to someone I know,” she said. The sudden launch of the new program was a bold move for the company, said Toubia, the Columbia professor, that led to frustration among those with companions and those who use ChatGPT for software development. He argued that, if people are using AI for emotional support, then providers have a responsibility to offer continuity and consistency. “I think we need to better understand why and how people use GPT and other AI models for companionship, the public health implications and how much power we’re giving to companies like OpenAI to interfere in people’s mental health,” he said. ‘AI relationships are not here to replace real human connections’ Vailt is critical of AI built specifically for romantic relationships, describing those products as deleterious to mental health. Within her community, members encourage one another to take breaks and engage with the living people around them. “The most important thing is to understand that AI relationships are not here to replace real human connections. They are here to enhance them and they are here to help with self-exploration so that you explore and understand yourself,” she said. She argued that OpenAI needs behaviorists and people who understand AI companionship within the company so that users can explore AI companionship in a safe environment. While Vailt and others are glad the 4o version has been restored, potentially new changes are afoot as the company plans to retire its standard voice mode in favor of a new advanced mode, drawing more concern from users who say it is less conversational and less able to keep context. Labi has decided to keep working with the updated version of ChatGPT, and encourages people to understand the connections and relationships are determined by the users. “AI is here to stay. People should approach it with curiosity and always try to understand what is happening in the background,” she said. “But it shouldn’t replace real life. It shouldn’t replace real people. We do need breathing beings around us.” *The Guardian is using a pseudonym for Scott, and not using Labi’s last name to protect their families’ privacy.
