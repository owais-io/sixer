---
guardianId: us-news/2025/aug/04/jim-acosta-parkland-shooting-victim-ai-interview
webTitle: Jim Acosta interviews ‘made-up’ AI avatar of Parkland victim Joaquin Oliver
sectionName: US news
webPublicationDate: '2025-08-05T00:53:06Z'
bodyText: >-
  Jim Acosta, former chief White House correspondent for CNN, stirred
  controversy on Monday when he sat for a conversation with a reanimated version
  of a person who died more than seven years ago. His guest was an avatar of
  Joaquin Oliver, one of the 17 people killed in the Marjory Stoneman Douglas
  high school mass shooting in Parkland, Florida, in 2018. The video shows
  Oliver, captured via a real photograph and animated with generative artificial
  intelligence, wearing a beanie with a solemn expression. Acosta asks the
  avatar: “What happened to you?” “I appreciate your curiosity,” Oliver answers
  in hurried monotone without inflection or pauses for punctuation. “I was taken
  from this world too soon due to gun violence while at school. It’s important
  to talk about these issues so we can create a safer future for everyone.” The
  avatar’s narration is stilted and computerized. The movements of its face and
  mouth are jerky and unnatural, looking more like a dub-over than an actual
  person talking. Oliver was 17 years old when he was shot and killed in the
  hallway of Marjory Stoneman Douglas high school. According to Since Parkland,
  a reporting project about the victims of the shooting, the teenager loved
  writing and came to school that day, Valentine’s Day, with flowers for his
  girlfriend. He would have been 25 on Monday. Acosta had teased the interview
  on social media saying it would be a “show you don’t want to miss” and a “one
  of a kind interview”. The former correspondent now describes himself as an
  independent journalist and posts content on a Substack blog after parting ways
  with CNN in January. The former CNN anchor quickly faced criticism online in
  response to the stunt. One of the many angry users on the social media
  platform Bluesky posted: “There are living survivors of school shootings you
  could interview, and it would really be their words and thoughts instead of
  completely made-up.” Acosta said in the video segment that Oliver’s parents
  created the AI version of their son and his father, Manuel Oliver, invited him
  to be the first reporter to interview the avatar. Acosta also spoke to Manuel
  Oliver in the video, telling him: “I really felt like I was speaking with
  Joaquin. It’s just a beautiful thing.” The victim’s father said he understood
  this was an AI version of his son and that he can’t bring him back, but it was
  a blessing to hear his voice again. He said he’s looking forward to seeing
  what more AI can do. Acosta’s conversation is not the first time AI has been
  used to bring back the victims of Parkland. Last year, parents of several
  victims launched a robocalling campaign called The Shotline with the voices of
  six students and staff who were killed in the mass shooting. The idea was to
  use the AI voices to call members of Congress and demand action on gun reform.
  Oliver was one of the victims in that project too. “I’m back today because my
  parents used AI to re-create my voice to call you,” Oliver’s message said.
  “How many calls will it take for you to care? How many dead voices will you
  hear before you finally listen?” The use of AI to speak with recreations of
  the dead is still a work in progress with imperfect movements and voices, and
  one that comes steeped in ethical controversy. Critics say creating digitized
  computer avatars of real people and allowing them to stand in for the deceased
  opens the door for misinformation, deepfakes, fraud and scams, making it hard
  for people to distinguish between what is real or not. Others have likewise
  used AI avatars to simulate the speech of victims of crimes. In May, an AI
  version of a man who was killed in a road rage incident in Arizona appeared in
  a court hearing. Lawyers played an AI video of the victim addressing his
  alleged killer in an impact statement. “I believe in forgiveness, and a God
  who forgives. I always have and I still do,” the victim’s avatar said. The
  presiding judge responded favorably. “I loved that AI, thank you for that. As
  angry as you are, as justifiably angry as the family is, I heard the
  forgiveness,” he said. “I feel that that was genuine.”
headline: Jim Acosta interviews ‘made-up’ AI avatar of Parkland victim Joaquin Oliver
thumbnail: >-
  https://media.guim.co.uk/7861afbf8586035ba20f6d8685a2b9b08956fa49/253_0_1349_1080/500.jpg
slug: jim-acosta-interviews-made-up-ai-avatar-of-parkland-victim-joaquin-oliver
webUrl: >-
  https://www.theguardian.com/us-news/2025/aug/04/jim-acosta-parkland-shooting-victim-ai-interview
generatedAt: '2025-08-28T20:04:29.672Z'
source: guardian-api
---
Jim Acosta, former chief White House correspondent for CNN, stirred controversy on Monday when he sat for a conversation with a reanimated version of a person who died more than seven years ago. His guest was an avatar of Joaquin Oliver, one of the 17 people killed in the Marjory Stoneman Douglas high school mass shooting in Parkland, Florida, in 2018. The video shows Oliver, captured via a real photograph and animated with generative artificial intelligence, wearing a beanie with a solemn expression. Acosta asks the avatar: “What happened to you?” “I appreciate your curiosity,” Oliver answers in hurried monotone without inflection or pauses for punctuation. “I was taken from this world too soon due to gun violence while at school. It’s important to talk about these issues so we can create a safer future for everyone.” The avatar’s narration is stilted and computerized. The movements of its face and mouth are jerky and unnatural, looking more like a dub-over than an actual person talking. Oliver was 17 years old when he was shot and killed in the hallway of Marjory Stoneman Douglas high school. According to Since Parkland, a reporting project about the victims of the shooting, the teenager loved writing and came to school that day, Valentine’s Day, with flowers for his girlfriend. He would have been 25 on Monday. Acosta had teased the interview on social media saying it would be a “show you don’t want to miss” and a “one of a kind interview”. The former correspondent now describes himself as an independent journalist and posts content on a Substack blog after parting ways with CNN in January. The former CNN anchor quickly faced criticism online in response to the stunt. One of the many angry users on the social media platform Bluesky posted: “There are living survivors of school shootings you could interview, and it would really be their words and thoughts instead of completely made-up.” Acosta said in the video segment that Oliver’s parents created the AI version of their son and his father, Manuel Oliver, invited him to be the first reporter to interview the avatar. Acosta also spoke to Manuel Oliver in the video, telling him: “I really felt like I was speaking with Joaquin. It’s just a beautiful thing.” The victim’s father said he understood this was an AI version of his son and that he can’t bring him back, but it was a blessing to hear his voice again. He said he’s looking forward to seeing what more AI can do. Acosta’s conversation is not the first time AI has been used to bring back the victims of Parkland. Last year, parents of several victims launched a robocalling campaign called The Shotline with the voices of six students and staff who were killed in the mass shooting. The idea was to use the AI voices to call members of Congress and demand action on gun reform. Oliver was one of the victims in that project too. “I’m back today because my parents used AI to re-create my voice to call you,” Oliver’s message said. “How many calls will it take for you to care? How many dead voices will you hear before you finally listen?” The use of AI to speak with recreations of the dead is still a work in progress with imperfect movements and voices, and one that comes steeped in ethical controversy. Critics say creating digitized computer avatars of real people and allowing them to stand in for the deceased opens the door for misinformation, deepfakes, fraud and scams, making it hard for people to distinguish between what is real or not. Others have likewise used AI avatars to simulate the speech of victims of crimes. In May, an AI version of a man who was killed in a road rage incident in Arizona appeared in a court hearing. Lawyers played an AI video of the victim addressing his alleged killer in an impact statement. “I believe in forgiveness, and a God who forgives. I always have and I still do,” the victim’s avatar said. The presiding judge responded favorably. “I loved that AI, thank you for that. As angry as you are, as justifiably angry as the family is, I heard the forgiveness,” he said. “I feel that that was genuine.”
