---
guardianId: commentisfree/2025/aug/03/generative-ai-chatbot-therapy-dangers-risks
webTitle: >-
  Using Generative AI for therapy might feel like a lifeline – but there’s
  danger in seeking certainty in a chatbot
sectionName: Opinion
webPublicationDate: '2025-08-03T15:00:44Z'
bodyText: >-
  Tran* sat across from me, phone in hand, scrolling. “I just wanted to make
  sure I didn’t say the wrong thing,” he explained, referring to a disagreement
  with his partner. “So I asked ChatGPT what I should say.” He read the
  chatbot-generated message aloud. It was articulate, logical and composed – too
  composed. It didn’t sound like Tran. And it definitely didn’t sound like
  someone in the middle of a complex emotional conversation about the future of
  a long-term relationship. It also did not mention anywhere some of Tran’s
  contributing behaviours to the relationship strain that Tran and I had been
  discussing. Like many others I’ve seen in therapy, Tran had turned to AI in a
  moment of crisis. Under immense pressure at work and facing uncertainty in his
  relationship, he’d downloaded ChatGPT on his phone “just to try it out”. What
  began as a curiosity soon became a daily habit, asking questions, drafting
  texts and even seeking reassurance about his own feelings. The more Tran used
  it, the more he began to second-guess himself in social situations, turning to
  the model for guidance before responding to colleagues or loved ones. He felt
  strangely comforted, like “no one knew me better”. His partner, on the other
  hand, began to feel as though she was talking to someone else entirely.
  ChatGPT and other generative AI models present a tempting accessory, or even
  alternative, to traditional therapy. They’re often free, available 24/7 and
  can offer customised, detailed responses in real time. When you’re
  overwhelmed, sleepless and desperate to make sense of a messy situation,
  typing a few sentences into a chatbot and getting back what feels like sage
  advice can be very appealing. But as a psychologist, I’m growing increasingly
  concerned about what I’m seeing in the clinic; a silent shift in how people
  are processing distress and a growing reliance on artificial intelligence in
  place of human connection and therapeutic support. AI might feel like a
  lifeline when services are overstretched – and make no mistake, services are
  overstretched. Globally, in 2019 one in eight people were living with a mental
  illness and we face a dire shortage of trained mental health professionals. In
  Australia, there has been a growing mental health workforce shortage that is
  limiting access to trained professionals. Clinician time is one of the
  scarcest resources in healthcare. It’s understandable (even expected) that
  people are looking for alternatives. But turning to a chatbot for emotional
  support isn’t without risk, especially when the lines between advice,
  reassurance and emotional dependence become blurred. Many psychologists,
  myself included, now encourage clients to build boundaries around their use of
  ChatGPT and similar tools. Its seductive “always-on” availability and friendly
  tone can unintentionally reinforce unhelpful behaviours, especially for people
  with anxiety, OCD or trauma-related issues. Reassurance-seeking, for example,
  is a key feature in OCD and ChatGPT, by design, provides reassurance in
  abundance. It never asks why you’re asking again. It never challenges
  avoidance. It never says, “Let’s sit with this feeling for a moment, and
  practise the skills we have been working on.” Tran often reworded prompts
  until the model gave him an answer that “felt right”. But this constant
  tailoring meant he wasn’t just seeking clarity; he was outsourcing emotional
  processing. Instead of learning to tolerate distress or explore nuance, he
  sought AI-generated certainty. Over time that made it harder for him to trust
  his own instincts. Beyond psychological concerns, there are real ethical
  issues. Information shared with ChatGPT isn’t protected by the same
  confidentiality standards as registered Ahpra professionals. Although OpenAI
  states that data from users is not used to train its models unless permission
  is given, the sheer volume of fine print in user agreements often goes unread.
  Users may not realise how their inputs can be stored, analysed and potentially
  reused. There’s also the risk of harmful or false information. These large
  language models are autoregressive; they predict the next word based on
  previous patterns. This probabilistic process can lead to “hallucinations”,
  confident, polished answers that are completely untrue. AI also reflects the
  biases embedded in its training data. Research shows that generative models
  can perpetuate and even amplify gender, racial and disability-based
  stereotypes – not intentionally, but unavoidably. Human therapists also
  possess clinical skills; we notice when a client’s voice trembles, or when
  their silence might say more than words. This isn’t to say AI can’t have a
  place. Like many technological advancements before it, generative AI is here
  to stay. It may offer useful summaries, psycho-educational content or even
  support in regions where access to mental health professionals is severely
  limited. But it must be used carefully, and never as a replacement for
  relational, regulated care. Tran wasn’t wrong to seek help. His instincts to
  make sense of distress and to communicate more thoughtfully were logical. But
  leaning so heavily on to AI meant that his skill development suffered. His
  partner began noticing a strange detachment in his messages. “It just didn’t
  sound like you,” she later told him. It turned out it wasn’t. She also became
  frustrated about the lack of accountability in his correspondence to her and
  this caused more relational friction and communication issues between them. As
  Tran and I worked together in therapy, we explored what led him to seek
  certainty in a chatbot. We unpacked his fears of disappointing others, his
  discomfort with emotional conflict and his belief that perfect words might
  prevent pain. Over time, he began writing his own responses, sometimes messy,
  sometimes unsure, but authentically his. Good therapy is relational. It
  thrives on imperfection, nuance and slow discovery. It involves pattern
  recognition, accountability and the kind of discomfort that leads to lasting
  change. A therapist doesn’t just answer; they ask and they challenge. They
  hold space, offer reflection and walk with you, while also offering up an
  uncomfortable mirror. For Tran, the shift wasn’t just about limiting his use
  of ChatGPT; it was about reclaiming his own voice. In the end he didn’t need a
  perfect response. He needed to believe that he could navigate life’s messiness
  with curiosity, courage and care – not perfect scripts. • *Name and
  identifying details changed to protect client confidentiality • Carly Dober is
  a psychologist living and working in Naarm/Melbourne • In Australia, support
  is available at Beyond Blue on 1300 22 4636, Lifeline on 13 11 14, and at
  MensLine on 1300 789 978. In the UK, the charity Mind is available on 0300 123
  3393 and Childline on 0800 1111. In the US, call or text Mental Health America
  at 988 or chat 988lifeline.org
headline: >-
  Using Generative AI for therapy might feel like a lifeline – but there’s
  danger in seeking certainty in a chatbot
thumbnail: >-
  https://media.guim.co.uk/96257b31eee33ef29f188d80d2894cff58dd07c7/2075_0_4884_3910/500.jpg
slug: >-
  using-generative-ai-for-therapy-might-feel-like-a-lifeline-but-theres-danger-in-seeking-certainty-in-a-chatbot
webUrl: >-
  https://www.theguardian.com/commentisfree/2025/aug/03/generative-ai-chatbot-therapy-dangers-risks
generatedAt: '2025-08-28T20:04:30.620Z'
source: guardian-api
---
Tran* sat across from me, phone in hand, scrolling. “I just wanted to make sure I didn’t say the wrong thing,” he explained, referring to a disagreement with his partner. “So I asked ChatGPT what I should say.” He read the chatbot-generated message aloud. It was articulate, logical and composed – too composed. It didn’t sound like Tran. And it definitely didn’t sound like someone in the middle of a complex emotional conversation about the future of a long-term relationship. It also did not mention anywhere some of Tran’s contributing behaviours to the relationship strain that Tran and I had been discussing. Like many others I’ve seen in therapy, Tran had turned to AI in a moment of crisis. Under immense pressure at work and facing uncertainty in his relationship, he’d downloaded ChatGPT on his phone “just to try it out”. What began as a curiosity soon became a daily habit, asking questions, drafting texts and even seeking reassurance about his own feelings. The more Tran used it, the more he began to second-guess himself in social situations, turning to the model for guidance before responding to colleagues or loved ones. He felt strangely comforted, like “no one knew me better”. His partner, on the other hand, began to feel as though she was talking to someone else entirely. ChatGPT and other generative AI models present a tempting accessory, or even alternative, to traditional therapy. They’re often free, available 24/7 and can offer customised, detailed responses in real time. When you’re overwhelmed, sleepless and desperate to make sense of a messy situation, typing a few sentences into a chatbot and getting back what feels like sage advice can be very appealing. But as a psychologist, I’m growing increasingly concerned about what I’m seeing in the clinic; a silent shift in how people are processing distress and a growing reliance on artificial intelligence in place of human connection and therapeutic support. AI might feel like a lifeline when services are overstretched – and make no mistake, services are overstretched. Globally, in 2019 one in eight people were living with a mental illness and we face a dire shortage of trained mental health professionals. In Australia, there has been a growing mental health workforce shortage that is limiting access to trained professionals. Clinician time is one of the scarcest resources in healthcare. It’s understandable (even expected) that people are looking for alternatives. But turning to a chatbot for emotional support isn’t without risk, especially when the lines between advice, reassurance and emotional dependence become blurred. Many psychologists, myself included, now encourage clients to build boundaries around their use of ChatGPT and similar tools. Its seductive “always-on” availability and friendly tone can unintentionally reinforce unhelpful behaviours, especially for people with anxiety, OCD or trauma-related issues. Reassurance-seeking, for example, is a key feature in OCD and ChatGPT, by design, provides reassurance in abundance. It never asks why you’re asking again. It never challenges avoidance. It never says, “Let’s sit with this feeling for a moment, and practise the skills we have been working on.” Tran often reworded prompts until the model gave him an answer that “felt right”. But this constant tailoring meant he wasn’t just seeking clarity; he was outsourcing emotional processing. Instead of learning to tolerate distress or explore nuance, he sought AI-generated certainty. Over time that made it harder for him to trust his own instincts. Beyond psychological concerns, there are real ethical issues. Information shared with ChatGPT isn’t protected by the same confidentiality standards as registered Ahpra professionals. Although OpenAI states that data from users is not used to train its models unless permission is given, the sheer volume of fine print in user agreements often goes unread. Users may not realise how their inputs can be stored, analysed and potentially reused. There’s also the risk of harmful or false information. These large language models are autoregressive; they predict the next word based on previous patterns. This probabilistic process can lead to “hallucinations”, confident, polished answers that are completely untrue. AI also reflects the biases embedded in its training data. Research shows that generative models can perpetuate and even amplify gender, racial and disability-based stereotypes – not intentionally, but unavoidably. Human therapists also possess clinical skills; we notice when a client’s voice trembles, or when their silence might say more than words. This isn’t to say AI can’t have a place. Like many technological advancements before it, generative AI is here to stay. It may offer useful summaries, psycho-educational content or even support in regions where access to mental health professionals is severely limited. But it must be used carefully, and never as a replacement for relational, regulated care. Tran wasn’t wrong to seek help. His instincts to make sense of distress and to communicate more thoughtfully were logical. But leaning so heavily on to AI meant that his skill development suffered. His partner began noticing a strange detachment in his messages. “It just didn’t sound like you,” she later told him. It turned out it wasn’t. She also became frustrated about the lack of accountability in his correspondence to her and this caused more relational friction and communication issues between them. As Tran and I worked together in therapy, we explored what led him to seek certainty in a chatbot. We unpacked his fears of disappointing others, his discomfort with emotional conflict and his belief that perfect words might prevent pain. Over time, he began writing his own responses, sometimes messy, sometimes unsure, but authentically his. Good therapy is relational. It thrives on imperfection, nuance and slow discovery. It involves pattern recognition, accountability and the kind of discomfort that leads to lasting change. A therapist doesn’t just answer; they ask and they challenge. They hold space, offer reflection and walk with you, while also offering up an uncomfortable mirror. For Tran, the shift wasn’t just about limiting his use of ChatGPT; it was about reclaiming his own voice. In the end he didn’t need a perfect response. He needed to believe that he could navigate life’s messiness with curiosity, courage and care – not perfect scripts. • *Name and identifying details changed to protect client confidentiality • Carly Dober is a psychologist living and working in Naarm/Melbourne • In Australia, support is available at Beyond Blue on 1300 22 4636, Lifeline on 13 11 14, and at MensLine on 1300 789 978. In the UK, the charity Mind is available on 0300 123 3393 and Childline on 0800 1111. In the US, call or text Mental Health America at 988 or chat 988lifeline.org
