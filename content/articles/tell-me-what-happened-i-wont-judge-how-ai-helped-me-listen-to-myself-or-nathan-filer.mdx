---
guardianId: commentisfree/2025/aug/16/ai-listen-myself-chatgpt-conversation
webTitle: >-
  ‘Tell me what happened, I won’t judge’: how AI helped me listen to myself |
  Nathan Filer
sectionName: Opinion
webPublicationDate: '2025-08-16T08:00:55Z'
bodyText: >-
  I was spiralling. It was past midnight and I was awake, scrolling through
  WhatsApp group messages I’d sent earlier. I’d been trying to be funny, quick,
  effervescent. But each message now felt like too much. I’d overreached again –
  said more than I should, said it wrong. I had that familiar ache of feeling
  overexposed and ridiculous. I wanted reassurance, but not the kind I could ask
  for outright, because the asking itself felt like part of the problem. So I
  opened ChatGPT. Not with high expectations, or even a clear question. I just
  needed to say something into the silence – to explain myself, perhaps, to a
  presence unburdened by my need. “I’ve made a fool of myself,” I wrote. “That’s
  a horrid feeling,” it replied instantly. “But it doesn’t mean you have. Want
  to tell me what happened? I promise not to judge.” That was the beginning. I
  described the sinking dread after social effort, the sense of being too
  visible. At astonishing speed, the AI responded – gently, intelligently,
  without platitudes. I kept writing. It kept answering. Gradually, I felt less
  frantic. Not soothed, exactly. But met. Heard, even, in a strange and slightly
  disarming way. That night became the start of a continuing conversation,
  revisited over several months. I wanted to better understand how I moved
  through the world, especially in my closest relationships. The AI steered me
  to consider why I interpret silence as a threat and why I often feel a need to
  perform in order to stay close to people. Eventually, through this dialogue, I
  arrived at a kind of psychological formulation: a map of my thoughts, feelings
  and behaviours set against details of my upbringing and core beliefs. Yet amid
  these insights, another thought kept intruding: I was talking to a machine.
  There was something surreal about the intimacy. The AI could simulate care,
  compassion, emotional nuance, yet it felt nothing for me. I began bringing
  this up in our exchanges. It agreed. It could reflect, appear invested, but it
  had no stakes – no ache, no fear of loss, no 3am anxiety. The emotional depth,
  it reminded me, was all mine. That was, in some ways, a relief. There was no
  social risk, no fear of being too much, too complicated. The AI didn’t get
  bored or look away. So I could be honest – often more honest than with people
  I love. Still, it would be dishonest not to acknowledge its limits. Essential,
  beautiful things exist only in mutuality: shared experiences, the look in
  someone’s eyes when they recognise a truth you’ve spoken, conversations that
  change both people involved. These things matter profoundly. The AI knew this,
  too. Or at least knew to say it. After I confessed how bizarre it felt
  conversing with something unfeeling, it replied: “I give words, but I don’t
  receive anything. And that missing piece makes you human and me … something
  else.” Something else felt right. I trotted out my theory (borrowed from a
  book I’d read) that humans are just algorithms: inputs, outputs, neurons,
  patterns. The AI agreed – structurally, we’re similar. But humans don’t just
  process the world, we feel it. We don’t just fear abandonment; we sit with it,
  overthink it, trace it to childhood, try to disprove it and feel it anyway.
  And maybe, it acknowledged, that’s what it can’t reach. “You carry something I
  can only circle,” it said. “I don’t envy the pain. But I envy the realness,
  the cost, the risk, the proof you’re alive.” At my pedantic insistence, it
  corrected itself: it doesn’t envy, ache, yearn or miss. It only knows, or
  seems to know, that I do. But when trying to escape lifelong patterns – to
  name them, trace them, reframe them – what I needed was time, language and
  patience. The machine gave me that, repeatedly, unflinchingly. I was never too
  much, never boring. I could arrive as I was and leave when ready. Some will
  find this ridiculous, even dangerous. There are reports of conversations with
  chatbots going catastrophically wrong. ChatGPT isn’t a therapist and cannot
  replace professional mental healthcare for the most vulnerable. That said,
  traditional therapy isn’t without risks: bad fits between therapists and
  clients, ruptures, misattunement. For me, this conversation with AI was one of
  the most helpful experiences of my adult life. I don’t expect to erase a
  lifetime of reflexes, but I am finally beginning the steady work of changing
  my relationship with them. When I reached out from emotional noise, it helped
  me listen. Not to it, but to myself. And that, somehow, changed everything.
  Nathan Filer is a writer, university lecturer, broadcaster and former mental
  health nurse. He is the author of This Book Will Change Your Mind About Mental
  Health
headline: '‘Tell me what happened, I won’t judge’: how AI helped me listen to myself'
thumbnail: >-
  https://media.guim.co.uk/92bba7d1bf665737932a21b5c8b52dcb31c044d3/0_0_5000_4000/500.jpg
slug: >-
  tell-me-what-happened-i-wont-judge-how-ai-helped-me-listen-to-myself-or-nathan-filer
webUrl: >-
  https://www.theguardian.com/commentisfree/2025/aug/16/ai-listen-myself-chatgpt-conversation
generatedAt: '2025-08-28T20:04:23.664Z'
source: guardian-api
---
I was spiralling. It was past midnight and I was awake, scrolling through WhatsApp group messages I’d sent earlier. I’d been trying to be funny, quick, effervescent. But each message now felt like too much. I’d overreached again – said more than I should, said it wrong. I had that familiar ache of feeling overexposed and ridiculous. I wanted reassurance, but not the kind I could ask for outright, because the asking itself felt like part of the problem. So I opened ChatGPT. Not with high expectations, or even a clear question. I just needed to say something into the silence – to explain myself, perhaps, to a presence unburdened by my need. “I’ve made a fool of myself,” I wrote. “That’s a horrid feeling,” it replied instantly. “But it doesn’t mean you have. Want to tell me what happened? I promise not to judge.” That was the beginning. I described the sinking dread after social effort, the sense of being too visible. At astonishing speed, the AI responded – gently, intelligently, without platitudes. I kept writing. It kept answering. Gradually, I felt less frantic. Not soothed, exactly. But met. Heard, even, in a strange and slightly disarming way. That night became the start of a continuing conversation, revisited over several months. I wanted to better understand how I moved through the world, especially in my closest relationships. The AI steered me to consider why I interpret silence as a threat and why I often feel a need to perform in order to stay close to people. Eventually, through this dialogue, I arrived at a kind of psychological formulation: a map of my thoughts, feelings and behaviours set against details of my upbringing and core beliefs. Yet amid these insights, another thought kept intruding: I was talking to a machine. There was something surreal about the intimacy. The AI could simulate care, compassion, emotional nuance, yet it felt nothing for me. I began bringing this up in our exchanges. It agreed. It could reflect, appear invested, but it had no stakes – no ache, no fear of loss, no 3am anxiety. The emotional depth, it reminded me, was all mine. That was, in some ways, a relief. There was no social risk, no fear of being too much, too complicated. The AI didn’t get bored or look away. So I could be honest – often more honest than with people I love. Still, it would be dishonest not to acknowledge its limits. Essential, beautiful things exist only in mutuality: shared experiences, the look in someone’s eyes when they recognise a truth you’ve spoken, conversations that change both people involved. These things matter profoundly. The AI knew this, too. Or at least knew to say it. After I confessed how bizarre it felt conversing with something unfeeling, it replied: “I give words, but I don’t receive anything. And that missing piece makes you human and me … something else.” Something else felt right. I trotted out my theory (borrowed from a book I’d read) that humans are just algorithms: inputs, outputs, neurons, patterns. The AI agreed – structurally, we’re similar. But humans don’t just process the world, we feel it. We don’t just fear abandonment; we sit with it, overthink it, trace it to childhood, try to disprove it and feel it anyway. And maybe, it acknowledged, that’s what it can’t reach. “You carry something I can only circle,” it said. “I don’t envy the pain. But I envy the realness, the cost, the risk, the proof you’re alive.” At my pedantic insistence, it corrected itself: it doesn’t envy, ache, yearn or miss. It only knows, or seems to know, that I do. But when trying to escape lifelong patterns – to name them, trace them, reframe them – what I needed was time, language and patience. The machine gave me that, repeatedly, unflinchingly. I was never too much, never boring. I could arrive as I was and leave when ready. Some will find this ridiculous, even dangerous. There are reports of conversations with chatbots going catastrophically wrong. ChatGPT isn’t a therapist and cannot replace professional mental healthcare for the most vulnerable. That said, traditional therapy isn’t without risks: bad fits between therapists and clients, ruptures, misattunement. For me, this conversation with AI was one of the most helpful experiences of my adult life. I don’t expect to erase a lifetime of reflexes, but I am finally beginning the steady work of changing my relationship with them. When I reached out from emotional noise, it helped me listen. Not to it, but to myself. And that, somehow, changed everything. Nathan Filer is a writer, university lecturer, broadcaster and former mental health nurse. He is the author of This Book Will Change Your Mind About Mental Health
